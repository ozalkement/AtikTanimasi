# -*- coding: utf-8 -*-
"""MobileNET with transfer learning_VET.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12WWbcNhMf-Wd9NaRabH2ySzuk5z7KWAB

<table>
  <tr>
    <td>
      <img src="https://drive.google.com/uc?id=1KFkyuUnR5e4_7oepa_ql0yDo7Nz4mLjR" alt="Project Logo"/>
    </td>
    <td>
      <h1><b>Image Classification with MobileNet (With Transfer Learning)</b></h1>
    </td>
  </tr>
</table>

#Problem

Our goal is to build an image classification model to identify three types of flowers: roses, sunflowers, and tulips.

ImageNet is a dataset commonly used for training image classification models, and MobileNet models are often pre-trained on it. However, ImageNet does not include specific categories for roses, sunflowers, or tulips. Because of this, a MobileNet model pre-trained on ImageNet cannot directly classify these flower types.

To classify roses, sunflowers, and tulips, the following approach will be applied:

1- Create a Custom Dataset gathering and label images of roses, sunflowers, and tulips.

2-Use the pre-trained MobileNet model and adapt (transfer learning) it by training on the custom flower dataset.

#Before you start

1.   Open the link https://drive.google.com/drive/folders/17VJCESBy5RFfisZBeCxPwu7vXQOKW_QB?usp=drive_link
2.   Open the notebook "*MobileNET with transfer learning.ipynb*" and save in your drive.
3.   In your drive create the folder "**TrainingGreeceCV**"
3.   Move the notebook to the folder that you have created.
5.   Downlod the folder "**data**" and "**SupportImages**" available on https://drive.google.com/drive/folders/17VJCESBy5RFfisZBeCxPwu7vXQOKW_QB?usp=drive_link
6.   Upload to the created folder "TrainingGreeceCV"
7.   Create the "models" inside the folder "TrainingGreeceCV"
7.   Activate the Table of contents (Top Menu>View>Table of contents).

###Mounting the Drive
This step is necessary in Colab because it operates in a temporary virtual environment, meaning that files saved directly in the Colab VM are deleted once the session ends. This integration also provides easy access to shared folders or files in Google Drive, making collaboration more efficient.

<p>&nbsp;</p>
"""

from google.colab import drive  # Import the drive module from Google Colab to access Google Drive
drive.mount('/content/drive')   # Mount Google Drive to the Colab environment at the specified path

"""```
# This is formatted as code
```

#Data collection

The first step in building a flower classification model is **data collection**.


When preparing a dataset for training with MobileNetV3 (the model that will be used), the directory structure and data organization should follow a specific format:


<p align="center">
  <img src="https://drive.google.com/uc?id=1FWmmKtCoT1VIubi5Sl_igJJw87GxcJS5" alt="img" width="600"/>

Each class folder (e.g., class_1, class_2, etc.) should contain the images belonging to that class. This structure allows training libraries to automatically assign labels based on folder names.

Also, we need to make sure that the source a dataset has enough images for each flower type to allow the model to learn the features of each category.

At this stage, we have two options:

1- Using an existing dataset can save time and effort, as they that are already labeled and organized for machine learning. These datasets are often structured in a way compatible with popular frameworks, so they’re easy to use.

Sometimes we cannot use an existing dataset because it may not meet the exact needs of our project. For example,the dataset might be missing images for specific classes we need or could include classes that are not relevant to our goals.

2- Then we need ti use our own collection of images, we can organize the images into a directory structure that meets the training requirements of MobileNetV3. This involves creating a main directory with subdirectories for each flower type. Each subdirectory will contain images for that specific flower, allowing our model to learn the differences between classes.



For this project, we will use a dataset available on [Kaggle Flowers Dataset](https://www.kaggle.com/datasets/rahmasleam/flowers-dataset/data):

Kaggle is an online platform for data science and machine learning, offering datasets, coding environments and competitions. It provides tools for sharing code, collaborating on projects, and accessing public datasets, making it a popular resource for data science learning and experimentation.

<p align="center">
  <img src="https://drive.google.com/uc?id=1tEypg2fe_p7U5NcaWAZwDZ1Xhx5CjNsf" alt="img" width="600"/>



This dataset contains labeled images of a set of flowers, including categories for roses, sunflowers and tulips.

The dataset is organized by flower type, making it suitable for direct use in model training. Images are sorted into folders by flower type, allowing easy loading and preprocessing.


1.   Click the "Download" button on the Kaggle dataset page.
2.   After downloading, unzip the file and select only the folders for roses, sunflowers, and tulips.
3.   Upload these folders to **TrainingGreeceCV/datasets/flowers** in Google Drive (create the folders if it doesn’t exist).


At the end, your floder structure should look like this:

<p align="center">
  <img src="https://drive.google.com/uc?id=1LPDc_aoXfoaD-Rb6zTSnpK5Jm9zoMtjM" alt="img" width="400"/>


You can check that each folder has images for each flower type — roses, sunflowers and tulips.

#Checking and Handling Imbalanced Data

The next step is to check for balanced data as part of the data preparation and preprocessing stage. This check should ideally occur after loading and organizing the dataset but before training begins, meaning this is the moment to do it.

Verifying data balance is essential because an imbalanced dataset — where some classes have significantly more images than others — can lead to a biased model that favors the overrepresented classes.

A balanced dataset ensures that the model has equal opportunity to learn the features of each class, improving its ability to classify each category accurately.

If we find that the dataset is imbalanced, we can take corrective actions, such as augmenting images in underrepresented classes or applying class weights during training, to address this issue before the model starts learning.
"""

import os
from collections import Counter
import matplotlib.pyplot as plt

# Path to the main dataset directory
data_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atik/'

# Count images in each subdirectory (i.e., each class)
class_counts = Counter()
for class_name in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_name)
    if os.path.isdir(class_path):
        class_counts[class_name] = len(os.listdir(class_path))
# Print the number of images in each class
for class_name, count in class_counts.items():
    print(f"Class '{class_name}': {count} images")

# Plotting the distribution of images per class
plt.figure(figsize=(8, 6))
plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')
plt.xlabel('Class')
plt.ylabel('Number of Images')
plt.title('Class Distribution in Dataset')
plt.show()

from google.colab import drive
drive.mount('/content/drive')

"""The dataset we are using is somewhat imbalanced, meaning that certain flower classes have more images than others, as showed on the graph.

An imbalance can affect the model's learning, making it more likely to favor the classes with more images, which can lead to biased predictions.

One approach to handling this imbalance would be to delete images from the overrepresented classes to make the dataset sizes equal across classes. However, removing images would reduce the amount of information the model can learn from, which would be a loss in terms of data quality and diversity.

Instead of deleting images, we will use **data augmentation** to balance the dataset. Data augmentation is a technique that generates new images by applying transformations to existing images. This allows us to create more samples of the underrepresented classes without gathering new images.

Common transformations used in data augmentation include:
1. Flipping (horizontal or vertical)
2. Rotation
3. Zooming
4. Color Adjustments


To perform data augmentation on the dataset, we will use TensorFlow’s ImageDataGenerator to generate and save augmented images for the underrepresented classes. This code will apply transformations like rotations, flips, and zooms to create more images, which will help balance the dataset.

"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

# Path to the main dataset directory and target number of images per class
data_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atik/'
target_num_images = 2000  # Desired images per class



# Define data augmentation transformations
datagen = ImageDataGenerator(      # Initialize an ImageDataGenerator instance for augmenting images
    rotation_range=40,             # Randomly rotate images within a 40-degree range to create varied angles
    width_shift_range=0.2,         # Randomly shift images horizontally by up to 20% of the width
    height_shift_range=0.2,        # Randomly shift images vertically by up to 20% of the height
    shear_range=0.2,               # Apply random shearing transformations with a 20% range for variety
    zoom_range=0.2,                # Randomly zoom in on images by up to 20% to create different scales
    horizontal_flip=True,          # Randomly flip images horizontally to create mirror images
    fill_mode='nearest'            # Fill any newly created pixels after transformations with nearest pixel values
)

# Loop through each class folder
for class_name in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_name)

    # Check if this is a folder and count current images
    if os.path.isdir(class_path):
        current_num_images = len(os.listdir(class_path))
        images_needed = target_num_images - current_num_images

        # Only augment if more images are needed
        if images_needed > 0:
            # Generate and save augmented images in the same class folder
            generator = datagen.flow_from_directory(
                data_dir,
                classes=[class_name],
                target_size=(224, 224),
                batch_size=1,
                save_to_dir=class_path,  # Save augmented images in the same folder as original images
                save_prefix='augmented',
                save_format='jpeg'
            )

            # Generate the exact number of images needed
            for _ in range(images_needed):
                next(generator)

import os
from collections import Counter
import matplotlib.pyplot as plt

# Path to the main dataset directory
data_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atik/'

# Count images in each subdirectory (i.e., each class)
class_counts = Counter()
for class_name in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_name)
    if os.path.isdir(class_path):
        class_counts[class_name] = len(os.listdir(class_path))
# Print the number of images in each class
for class_name, count in class_counts.items():
    print(f"Class '{class_name}': {count} images")

# Plotting the distribution of images per class
plt.figure(figsize=(8, 6))
plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')
plt.xlabel('Class')
plt.ylabel('Number of Images')
plt.title('Class Distribution in Dataset')
plt.show()

"""Now, we can execute the same code as before to apply data augmentation and ensure that the classes are balanced.

"""

import os
from collections import Counter
import matplotlib.pyplot as plt

# Path to the main dataset directory
data_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atik/'

# Count images in each subdirectory (i.e., each class)
class_counts = Counter()
for class_name in os.listdir(data_dir):
    class_path = os.path.join(data_dir, class_name)
    if os.path.isdir(class_path):
        class_counts[class_name] = len(os.listdir(class_path))

# Plotting the distribution of images per class
plt.figure(figsize=(8, 6))
plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')
plt.xlabel('Class')
plt.ylabel('Number of Images')
plt.title('Class Distribution in Dataset')
plt.show()

"""#Organizing the Dataset for Training

Organizing the Dataset for Training means arranging data in a structured way to align with the requirements of the machine learning model. Different models require different **dataset** structures due to their different architectures and training approaches.

For this project, we are using MobileNetV3. MobileNet’s structure is relatively simple compared to other models and it requires a straightforward dataset structure.

MobileNet expects images to be organized by class, where each class has its own folder containing all its images.

Example of the structure:


<p align="center">
  <img src="https://drive.google.com/uc?id=1PMc4NiW8cpERoSEQRgeg_l2nndSN70MD" alt="img" width="400"/>

In MobileNet, folder names are used to automatically label images, with each folder representing a specific class. The model associates all images in a folder with that class label, eliminating the need for separate label files.

Splitting a dataset into *training*, *validation* and *test* sets helps the model learn patterns from one part of the data, fine-tune its parameters, and evaluate its performance on unseen data.

**Training Set**: This portion, typically 70% of the data, is used by the model to learn patterns, features, and relationships in each class, providing it with enough examples to build a robust understanding.

**Validation Set**: Around 15% of the data is used for validation, allowing the model to adjust its parameters and avoid overfitting on the training data. The validation set helps to tune the model without influencing the final performance measurement.

**Test Set**: The remaining 15% is kept for testing. This data is not seen by the model during training or validation, allowing an unbiased evaluation of how well the model performs on new data. A 15% test set typically provides a reliable estimate of the model’s real-world accuracy.


This process is very important because:

**Preventing Overfitting**
If the model trains and tests on the same data, it may “memorize” the dataset instead of learning patterns that apply to new data. Splitting the data helps the model learn in a way that prepares it for new examples.

**Reliable Evaluation**
The test set provides a way to check the model’s accuracy and other metrics on data it hasn’t seen before. This helps ensure the model’s performance is realistic and not just good on familiar data.

**Better Generalization**
The model trains on a variety of examples but is tested on separate data. This setup helps the model learn patterns that work well on new data rather than just the training examples.


"""

import os
import shutil
import random

# Define the source directory and target directories for train and test
source_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atik/'
train_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/train/'
test_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/test/'
val_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/val/'

# Define split ratio
train_ratio = 0.7  # 70% for training
test_ratio = 0.15   # 15% for testing
validation_ratio = 0.15   # 15% for validation


# Define the split ratios
train_ratio = 0.7
validation_ratio = 0.15
test_ratio = 0.15


# Create target directories if they don't exist
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Iterate through each class folder in the source directory
for class_name in os.listdir(source_dir):
    class_path = os.path.join(source_dir, class_name)
    if os.path.isdir(class_path):
        # Create subdirectories in train, validation, and test folders for each class
        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)
        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)

        # Get list of all images in the class folder and shuffle them
        images = os.listdir(class_path)
        random.shuffle(images)

        # Calculate split counts
        train_count = int(len(images) * train_ratio)
        validation_count = int(len(images) * validation_ratio)

        # Split images into train, validation, and test sets
        train_images = images[:train_count]
        validation_images = images[train_count:train_count + validation_count]
        test_images = images[train_count + validation_count:]

        # Move images to the train folder
        for img in train_images:
            src_path = os.path.join(class_path, img)
            dst_path = os.path.join(train_dir, class_name, img)
            shutil.copy(src_path, dst_path)

        # Move images to the validation folder
        for img in validation_images:
            src_path = os.path.join(class_path, img)
            dst_path = os.path.join(val_dir, class_name, img)
            shutil.copy(src_path, dst_path)

        # Move images to the test folder
        for img in test_images:
            src_path = os.path.join(class_path, img)
            dst_path = os.path.join(test_dir, class_name, img)
            shutil.copy(src_path, dst_path)

print("Dataset successfully split into train, validation, and test sets.")

from google.colab import drive
drive.mount('/content/drive')

"""

Check the number of files"""

def count_images(directory):
    total_count = 0
    for class_name in os.listdir(directory):
        class_folder = os.path.join(directory, class_name)
        if os.path.isdir(class_folder):
            num_images = len(os.listdir(class_folder))
            total_count += num_images
            print(f"{class_name} - {directory.split('/')[-1]}: {num_images} images")
    print(f"Total images in {directory.split('/')[-1]} set: {total_count}\n")

print("\nCounting images in each set:")
count_images(train_dir)
count_images(val_dir)
count_images(test_dir)

"""Now, we are ready to code!

#Import the libraries
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV3Small
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

"""#Set Up Data Generators

Setting up data generators is essential for training machine learning models for several reasons.

First, data generators manage efficient data loading and memory by handling images in batches, which prevents memory overflow. Instead of loading all images at once, data generators load only a few images at a time, allowing training on large datasets without using too much memory.

Second, data generators enable real-time data augmentation, which helps improve model robustness, expanding the dataset through transformations like rotations, shifts, zooms, and flips. This means the model sees slightly different versions of each image during each training epoch, which helps reduce overfitting and improves the model's ability to handle variations in new data.

Third, data generators automatically assign labels and perform basic preprocessing, such as resizing and normalizing images, based on the folder structure.

"""

# Define directories for training and validation datasets
train_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/train/'
val_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/val/'


# Set up ImageDataGenerators for training and testing with data augmentation
train_datagen = ImageDataGenerator(rotation_range=15)
train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32)


val_datagen = ImageDataGenerator(rotation_range=15)
val_generator = val_datagen.flow_from_directory(val_dir, target_size=(224, 224), batch_size=32)

"""The number of images created in this stage through data augmentation depends on the number of epochs and batches used during training, rather than a fixed count.

Data augmentation generates new variations of images in each batch, producing an "infinite" supply of training data without actually increasing the physical dataset size. When using an ImageDataGenerator with augmentation, each batch provides a modified "view" of the original images, with transformations like rotation, zoom, and shifts. This means that the total number of unique image variations the model encounters is determined by the number of epochs and batches.

These augmentations should be used carefully because excessive or inappropriate transformations can distort the images, leading to poor model learning. For example, extreme rotations, zooms, or flips may create unrealistic versions of images that do not represent the actual classes, confusing the model instead of improving it. Additionally, if certain augmentations (like flips) are applied to classes where orientation matters, the model may struggle to distinguish between correct and incorrect orientations, reducing accuracy. Properly balanced augmentations help the model generalize by introducing useful variations, but overuse or misapplication can negatively affect its performance on real-world data.

#Load Pre-trained MobileNetV3 and Modify for Transfer Learning

We’ll load the MobileNetV3 model, remove its final classification layer, and add new layers for our custom classes. Freeze the pre-trained layers to retain learned features and add a custom output layer for our dataset’s specific classes.

This following code creates a custom classification model based on MobileNetV3Small, a pre-trained model, which is used as a feature extractor by setting include_top=False to remove its original classification layer.

The base model outputs feature maps, which are then passed through a series of custom layers: a global average pooling layer to reduce dimensionality and three fully connected layers with ReLU activation to help the model learn complex patterns.

The final output layer has 3 units with a softmax activation, enabling multi-class classification for three categories. The model is configured by connecting the base model’s input to this new output layer.

All layers except the last five are frozen, so only the new layers are updated during training, which helps retain MobileNet’s learned features and reduces the likelihood of overfitting.
"""

baseModel = MobileNetV3Small(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

# Add custom layers on top of the base model
x = baseModel.output
x = GlobalAveragePooling2D()(x)                # Add global average pooling layer
x = Dense(512, activation='relu')(x)           # Add fully connected layer with 512 units
x = Dense(256, activation='relu')(x)           # Add fully connected layer with 256 units
x = Dense(128, activation='relu')(x)           # Add fully connected layer with 128 units

# Output layer with 3 classes and softmax activation for multi-class classification
predictionLayer = Dense(5, activation='softmax')(x)

# Define the full model by connecting base model input and custom prediction layer
model = Model(inputs=baseModel.input, outputs=predictionLayer)

# Freeze the layers of MobileNetV3 (already trained layers)
for layer in model.layers[:-5]:  # Freeze all layers except the last 5
    layer.trainable = False

"""#Compile the Model
To prepare the model for training, we need to compile it by specifying an optimizer, a loss function, and evaluation metrics.

**Optimizer**: The optimizer adjusts the model's weights during training to minimize the loss function. Here, we use the *Adam optimizer*, a popular choice for deep learning models, as it adapts the learning rate for each parameter individually, making training efficient. For transfer learning, we  set a smaller *learning rate* that allows fine-tuning of the new layers added on top of the pre-trained model without making drastic changes to the pre-trained weights. This approach helps retain the beneficial features learned from the original ImageNet dataset while adapting the model to new data.

**Loss Function**: The loss function measures how well the model's predictions match the true labels. We use *categorical crossentropy*, which compares the predicted probability distribution across classes with the true distribution.

**Metrics**: Metrics are used to evaluate the model's performance during training and testing. We  use *accuracy* as a primary metric for classification tasks, as it indicates the percentage of correctly classified images.


"""

# Compile the model with optimizer, loss function, and metrics
optimizer = Adam(learning_rate=0.0001)
model.compile(loss="categorical_crossentropy", optimizer=optimizer, metrics=['accuracy'])

"""#Train and save the Model

In Colab enviroment make sure that you have access to a T4 GPU (Graphics Processing Unit). The T4 GPU is used for deep learning and model training because it can handle the parallel computations required for tasks like matrix operations and tensor manipulations, which are common in neural networks.

To switch from CPU to GPU in Google Colab, follow these steps:

1- In your Colab notebook, go to the top menu and select Runtime and select Change runtime type.

<p align="center">
  <img src="https://drive.google.com/uc?id=1e9USDjThtQGqS6l5VqiMnDgtYlTMx-64" alt="img" width="400"/>

2- In the pop-up window, find the Hardware accelerator dropdown menu.
Select GPU from the dropdown options (you may also see None and TPU as other options). Click Save to confirm your selection.

<p align="center">
  <img src="https://drive.google.com/uc?id=1prfj8jufbeMesG3iyMe_dG0EN8PuzgaV" alt="img" width="400"/>


<p>
  <small><i>This task takes about 30 minutes.</i></small>
</p>


"""

model.fit(train_generator, validation_data=val_generator, epochs=10)

model.save('/content/drive/My Drive/Colab Notebooks/VET_Proje/models/atik_classifier_model_AUG_FV2.keras')

"""#Evaluate the model

The following code evaluates our model on the **test** set by loading the test data, predicting the classes, and calculating various performance metrics.

The test data is loaded with ImageDataGenerator in a non-augmented format, which feeds images to the model without shuffling to ensure consistent results.

The model evaluation step provides loss and accuracy metrics, where loss reflects the prediction error, and accuracy shows the proportion of correct predictions over the total samples.

- The **confusion matrix** visually displays the model's performance across each class, showing how often the model correctly or incorrectly predicted each class, which helps identify patterns of misclassification.

- **Precision** measures how many of the predicted positive results are actually positive (useful for reducing false positives)

- **recall** measures how many actual positives were correctly identified (useful for reducing false negatives)

- **F1 score** combines precision and recall to provide a balanced measure, especially helpful for imbalanced datasets.

The classification report gives a per-class summary of precision, recall, and F1 score, providing deeper insight into model performance for each class.

- **ROC curve** (Receiver Operating Characteristic curve) plots the true positive rate against the false positive rate, helping assess the model's ability to distinguish between classes, with the AUC (Area Under the Curve) score indicating overall classification performance—values closer to 1.0 suggest better distinction between classes.
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, classification_report
import matplotlib.pyplot as plt
import numpy as np


model = load_model('/content/drive/My Drive/Colab Notebooks/VET_Proje/models/atik_classifier_model_AUG_FV2.keras')


# Set up the ImageDataGenerator for the test set
# Define the test directory
test_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/test/'

# Set up ImageDataGenerator for the test data without augmentation
test_datagen = ImageDataGenerator()  # No data augmentation

# Create the test generator
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',  # Change to 'binary' if it's a binary classification task
    shuffle=False  # Don't shuffle test data
)

# Calculate the number of steps for evaluation based on the test generator
evaluation_steps = test_generator.samples // test_generator.batch_size

# Evaluate model to confirm loss and accuracy
loss, accuracy = model.evaluate(test_generator)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Generate predictions
predictions = model.predict(test_generator)
y_pred = np.argmax(predictions, axis=1)
y_true = test_generator.classes
y_pred_prob = predictions if predictions.shape[1] == 2 else predictions[:, 1]  # For ROC

# Confusion matrix
cm = confusion_matrix(y_true, y_pred)
class_names = list(test_generator.class_indices.keys())

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Compute precision, recall, and F1 scores
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')

# Print classification report
class_names = list(test_generator.class_indices.keys())
report = classification_report(y_true, y_pred, target_names=class_names)
print("Classification Report:\n", report)
print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

# ROC Curve Plotting
plt.figure(figsize=(8, 6))
if len(class_names) == 2:  # Binary classification
    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')
else:  # Multiclass classification
    for i in range(len(class_names)):
        fpr, tpr, _ = roc_curve(y_true == i, predictions[:, i])
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f'ROC curve of class {class_names[i]} (area = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
from tensorflow.keras.models import load_model

model = load_model('/content/drive/My Drive/Colab Notebooks/VET_Proje/models/atik_classifier_model_AUG_FV2.keras')
# Set up ImageDataGenerators for training and testing with data augmentation

train_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/train/'
train_datagen = ImageDataGenerator()
train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32)
train_dir = '/content/drive/My Drive/Colab Notebooks/VET_Proje/atikToTrain/train/'



# Path to the single image
img_path = '/content/drive/My Drive/Colab Notebooks/VET_Proje/c3.jpg'

# Load and preprocess the image
img = load_img(img_path, target_size=(224, 224))
img_array = img_to_array(img)
img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension

# Predict the class
prediction = model.predict(img_array)
predicted_class = np.argmax(prediction, axis=1)[0]
print(prediction)

# Map index to class label
class_labels = {v: k for k, v in train_generator.class_indices.items()}

# Print predictions for all classes with percentages
print("Predictions for all classes:")
for index, probability in enumerate(prediction[0]):
    label = class_labels[index]
    percentage = probability * 100  # Convert to percentage
    print(f"Class: {label}, Probability: {percentage:.2f}%")

# Print the top predicted class
predicted_class = np.argmax(prediction, axis=1)[0]
predicted_label = class_labels[predicted_class]
predicted_probability = prediction[0][predicted_class] * 100
print(f"\nPredicted class: {predicted_label} with confidence: {predicted_probability:.2f}%")

"""<p>&nbsp;</p>
<hr>
<p>&nbsp;</p>
<img src="https://drive.google.com/uc?id=1q41vpd1KosNcdWs-Inhb3E-o-E_t3FW-" alt="Project Logo" width="10%"/>
<img src="https://drive.google.com/uc?id=1OUkxiLL97JCkx6l4qs9RCSH-qHECxOKK" alt="Project Logo" width="20%">
<img src="https://drive.google.com/uc?id=1R5gOPRbcuQdFo91EusE2ZXjujHZnWPbq" alt="Project Logo" width="20%">
<img src="https://drive.google.com/uc?id=1MnQ7l9ytjsTV9RNdd-dEqLEs_M5pEUr8" alt="Project Logo" width="20%">
<img src="https://drive.google.com/uc?id=1KFkyuUnR5e4_7oepa_ql0yDo7Nz4mLjR" alt="Project Logo" width="20%">


"""